{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from deepseek_tokenizer import ds_token\n",
    "from ollama import chat\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_deepseek(query: str) -> None:\n",
    "    stream = chat(\n",
    "        model='deepseek-r1:1.5b',\n",
    "        messages=[{'role': 'user', 'content': query}],\n",
    "    )\n",
    "    return stream['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = \"\"\"\n",
    "Does this text include information about %s? If the information is available, print <total>1</total>; otherwise, print <total>0</total>.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text: str, chunk_size: int = 1024) -> list:\n",
    "    tokens = ds_token.encode(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for token in tokens:\n",
    "        if current_length + 1 > chunk_size:\n",
    "            chunks.append(ds_token.decode(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(token)\n",
    "        current_length += 1\n",
    "    if current_chunk:\n",
    "        chunks.append(ds_token.decode(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_file(filepath: str) -> None:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    with open(\"parameters.txt\") as f:\n",
    "        parameters = f.readlines()\n",
    "    chunks = split_text_into_chunks(content)\n",
    "    results = []\n",
    "    additional_run_results = []\n",
    "    for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "        result_dict = {}\n",
    "        result_dict['chunk'] = chunk\n",
    "        additional_run_dict = {}\n",
    "        additional_run_dict['chunk'] = chunk\n",
    "        for parameter in tqdm(parameters, desc=\"Processing parameters\", leave=False):\n",
    "            parameter = parameter.strip()\n",
    "            results_sum = 0\n",
    "            for _ in range(3):\n",
    "                raw_output = query_deepseek(BASE_PROMPT % parameter + chunk)\n",
    "                match = re.search(r'<total>(\\d+)</total>', raw_output)\n",
    "                if match:\n",
    "                    value = int(match.group(1))\n",
    "                    result = 1 if value > 1 else 0\n",
    "                else:\n",
    "                    result = 0\n",
    "                results_sum += result\n",
    "                additional_results_sum = 0\n",
    "                if results_sum > 1:\n",
    "                    for _ in range(3):\n",
    "                        raw_output = query_deepseek(BASE_PROMPT % parameter + chunk)\n",
    "                        match = re.search(r'<total>(\\d+)</total>', raw_output)\n",
    "                        if match:\n",
    "                            value = int(match.group(1))\n",
    "                            result = 1 if value > 1 else 0\n",
    "                        else:\n",
    "                            result = 0\n",
    "                        additional_results_sum += result\n",
    "                    additional_run_dict[parameter] = additional_results_sum\n",
    "            additional_run_dict[parameter] = additional_results_sum\n",
    "            result_dict[parameter] = results_sum\n",
    "        results.append(result_dict)\n",
    "        additional_run_results.append(additional_run_dict)\n",
    "    with open(filepath + \".results\", \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    with open(filepath + \".addl_results\", \"wb\") as f:\n",
    "        pickle.dump(additional_run_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_file('sample.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
